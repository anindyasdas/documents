# Main configuration.

siamese {
  context_dim = 300
  lstm_dim = 200
  pos_tags_n = 12
  epochs = 15
  batch_size = 150
}

bert{
TRAIN_BATCH_SIZE = 32
EVAL_BATCH_SIZE = 2
PREDICT_BATCH_SIZE = 2
LEARNING_RATE = 2e-5
NUM_TRAIN_EPOCHS = 3.0
MAX_SEQ_LENGTH = 128

WARMUP_PROPORTION = 0.1
# Model configs
SAVE_CHECKPOINTS_STEPS = 1000
SAVE_SUMMARY_STEPS = 500
}

ml_models{
k_fold = 5
}

siamese_bert {
  ensemble = true
  model_path = siam_bert_base/bert-base-nli-mean-tokens/
  read_json = true
}

info_extr_spec {
  top_idf_ratio = 0.9
  exclude_after = ["will", "read", "could", "may", "want", "fetch", "let", "can", "need", "takes", "facing", "get", "i", "need", "can", "what", "how", "why", "can", "is there", "could you", "please", "give", "will", "is this"]

}

info_extr_trob {
  top_idf_ratio = 0.9
  #exclude_after = ["will", "read", "could", "may", "want", "fetch", "let", "can", "need", "takes", "facing", "get", "need", "can", "what", "how", "why", "can", "is there", "could you", "please", "give", "will", "is this"]
  exclude_after = []
}
