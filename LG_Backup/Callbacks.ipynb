{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import math\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow callbacks are functions or blocks of code which are executed during a specific instant while training a Deep Learning Model. Once the training process is started there is no way to pause the training in case we want to change some params. Also, in some cases when the model has been trained for several hours and we want to tweak some parameters at the later stages, it is impossible to do so. This is where TensorFlow callbacks come to the rescue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train),(x_val, y_val) = mnist.load_data()\n",
    "x_train, x_val = x_train / 255.0, x_val / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def build_model():\n",
    "    inp = tf.keras.Input((28,28))\n",
    "    flattern_layer = tf.keras.layers.Flatten()(inp)\n",
    "    dense_1 = tf.keras.layers.Dense(512, activation='relu')(flattern_layer)\n",
    "    output_layer = tf.keras.layers.Dense(10, activation='softmax')(dense_1)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inp, outputs=output_layer)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 8s 171us/step - loss: 0.6086 - acc: 0.8075 - val_loss: 0.4245 - val_acc: 0.8479\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 7s 153us/step - loss: 0.4193 - acc: 0.8491 - val_loss: 0.4148 - val_acc: 0.8472\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 8s 157us/step - loss: 0.3947 - acc: 0.8571 - val_loss: 0.4390 - val_acc: 0.8520\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27490108a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "opt = tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=opt,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor=\"val_loss\",\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-2,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "47424/48000 [============================>.] - ETA: 0s - loss: 0.5625 - acc: 0.8105\n",
      "Epoch 00001: val_loss improved from inf to 0.47629, saving model to checkpoints/mymodel_1.h5\n",
      "48000/48000 [==============================] - 5s 96us/sample - loss: 0.5610 - acc: 0.8108 - val_loss: 0.4763 - val_acc: 0.8178\n",
      "Epoch 2/5\n",
      "47680/48000 [============================>.] - ETA: 0s - loss: 0.4228 - acc: 0.8482\n",
      "Epoch 00002: val_loss improved from 0.47629 to 0.40531, saving model to checkpoints/mymodel_2.h5\n",
      "48000/48000 [==============================] - 5s 98us/sample - loss: 0.4227 - acc: 0.8482 - val_loss: 0.4053 - val_acc: 0.8489\n",
      "Epoch 3/5\n",
      "47552/48000 [============================>.] - ETA: 0s - loss: 0.3846 - acc: 0.8606\n",
      "Epoch 00003: val_loss improved from 0.40531 to 0.37796, saving model to checkpoints/mymodel_3.h5\n",
      "48000/48000 [==============================] - 5s 102us/sample - loss: 0.3841 - acc: 0.8606 - val_loss: 0.3780 - val_acc: 0.8653\n",
      "Epoch 4/5\n",
      "47808/48000 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.8665\n",
      "Epoch 00004: val_loss did not improve from 0.37796\n",
      "48000/48000 [==============================] - 5s 110us/sample - loss: 0.3680 - acc: 0.8666 - val_loss: 0.4005 - val_acc: 0.8620\n",
      "Epoch 5/5\n",
      "47808/48000 [============================>.] - ETA: 0s - loss: 0.3573 - acc: 0.8700\n",
      "Epoch 00005: val_loss did not improve from 0.37796\n",
      "48000/48000 [==============================] - 5s 97us/sample - loss: 0.3574 - acc: 0.8700 - val_loss: 0.4206 - val_acc: 0.8472\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = build_model()\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        # The saved model name will include the current epoch.\n",
    "        filepath=\"checkpoints/mymodel_{epoch}.h5\",\n",
    "        save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "    )\n",
    "]\n",
    "history = model.fit(\n",
    "    x_train, y_train, epochs=5, batch_size=64, callbacks=callbacks, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.096\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 1s 157us/sample - loss: 11.5362 - acc: 0.6301 - val_loss: 1.0994 - val_acc: 0.7075\n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 1s 136us/sample - loss: 0.9301 - acc: 0.6901 - val_loss: 0.7668 - val_acc: 0.7350\n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 1s 156us/sample - loss: 0.7421 - acc: 0.7391 - val_loss: 0.7076 - val_acc: 0.7440\n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 2s 188us/sample - loss: 0.6051 - acc: 0.7930 - val_loss: 0.6837 - val_acc: 0.7820\n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 1s 147us/sample - loss: 0.5232 - acc: 0.8154 - val_loss: 0.6112 - val_acc: 0.8160\n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 1s 153us/sample - loss: 0.4765 - acc: 0.8326 - val_loss: 0.5953 - val_acc: 0.8175\n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 1s 161us/sample - loss: 0.4506 - acc: 0.8424 - val_loss: 0.6268 - val_acc: 0.8205\n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 1s 152us/sample - loss: 0.4394 - acc: 0.8490 - val_loss: 0.6052 - val_acc: 0.8225\n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 1s 154us/sample - loss: 0.4256 - acc: 0.8521 - val_loss: 0.6005 - val_acc: 0.8290\n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 1s 164us/sample - loss: 0.4191 - acc: 0.8554 - val_loss: 0.6060 - val_acc: 0.8270\n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 1s 154us/sample - loss: 0.4144 - acc: 0.8549 - val_loss: 0.6063 - val_acc: 0.8310\n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 1s 159us/sample - loss: 0.4116 - acc: 0.8555 - val_loss: 0.6054 - val_acc: 0.8305\n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 1s 170us/sample - loss: 0.4104 - acc: 0.8569 - val_loss: 0.6053 - val_acc: 0.8315\n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 1s 164us/sample - loss: 0.4092 - acc: 0.8575 - val_loss: 0.6055 - val_acc: 0.8305\n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 1s 158us/sample - loss: 0.4088 - acc: 0.8572 - val_loss: 0.6058 - val_acc: 0.8300\n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 1s 145us/sample - loss: 0.4083 - acc: 0.8576 - val_loss: 0.6060 - val_acc: 0.8310\n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 1s 145us/sample - loss: 0.4080 - acc: 0.8577 - val_loss: 0.6062 - val_acc: 0.8315\n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 1s 147us/sample - loss: 0.4079 - acc: 0.8579 - val_loss: 0.6061 - val_acc: 0.8315\n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 1s 142us/sample - loss: 0.4078 - acc: 0.8577 - val_loss: 0.6062 - val_acc: 0.8315\n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 1s 137us/sample - loss: 0.4077 - acc: 0.8579 - val_loss: 0.6062 - val_acc: 0.8315\n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 1s 150us/sample - loss: 0.4077 - acc: 0.8580 - val_loss: 0.6062 - val_acc: 0.8315\n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 1s 156us/sample - loss: 0.4077 - acc: 0.8580 - val_loss: 0.6062 - val_acc: 0.8315\n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 1s 151us/sample - loss: 0.4077 - acc: 0.8580 - val_loss: 0.6062 - val_acc: 0.8315\n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 1s 143us/sample - loss: 0.4077 - acc: 0.8580 - val_loss: 0.6062 - val_acc: 0.8315\n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 1s 147us/sample - loss: 0.4077 - acc: 0.8580 - val_loss: 0.6062 - val_acc: 0.8315\n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 1s 151us/sample - loss: 0.4076 - acc: 0.8580 - val_loss: 0.6062 - val_acc: 0.8315\n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 1s 158us/sample - loss: 0.4076 - acc: 0.8580 - val_loss: 0.6062 - val_acc: 0.8315\n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 1s 156us/sample - loss: 0.4076 - acc: 0.8580 - val_loss: 0.6062 - val_acc: 0.8315\n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 1s 148us/sample - loss: 0.4076 - acc: 0.8580 - val_loss: 0.6062 - val_acc: 0.8315\n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 1s 151us/sample - loss: 0.4076 - acc: 0.8580 - val_loss: 0.6062 - val_acc: 0.8315\n"
     ]
    }
   ],
   "source": [
    "initial_learning_rate = 0.1\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=10, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "print(0.1*0.96 )\n",
    "        \n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "model.compile(optimizer=opt,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    x_val, y_val, epochs=30, batch_size=64, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/1050/1*7xeRIjAhnih6EWKrOk_aKw.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/max/1050/1*7xeRIjAhnih6EWKrOk_aKw.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass a list of callbacks (as the keyword argument callbacks) to the following model methods:\n",
    "\n",
    "keras.Model.fit()\n",
    "keras.Model.evaluate()\n",
    "keras.Model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Learning rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 1s 179us/sample - loss: 4.1203 - acc: 0.6225 - val_loss: 1.8178 - val_acc: 0.5665\n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 1s 146us/sample - loss: 1.4995 - acc: 0.6062 - val_loss: 1.1510 - val_acc: 0.6075\n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 1s 175us/sample - loss: 1.5651 - acc: 0.6012 - val_loss: 1.2142 - val_acc: 0.6365\n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 1s 135us/sample - loss: 1.5893 - acc: 0.6024 - val_loss: 2.9332 - val_acc: 0.5830\n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 1s 128us/sample - loss: 1.4170 - acc: 0.6173 - val_loss: 1.3212 - val_acc: 0.6590\n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 1s 134us/sample - loss: 1.3006 - acc: 0.6226 - val_loss: 1.2410 - val_acc: 0.5505\n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 1s 136us/sample - loss: 1.4077 - acc: 0.6125 - val_loss: 1.1392 - val_acc: 0.5835\n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 1s 136us/sample - loss: 1.3513 - acc: 0.6034 - val_loss: 1.3717 - val_acc: 0.6110\n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 1s 154us/sample - loss: 1.3075 - acc: 0.6170 - val_loss: 1.1949 - val_acc: 0.6005\n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 1s 147us/sample - loss: 0.9741 - acc: 0.6571 - val_loss: 1.2257 - val_acc: 0.6420\n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 2s 191us/sample - loss: 0.9108 - acc: 0.6709 - val_loss: 1.1518 - val_acc: 0.6840\n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 1s 153us/sample - loss: 0.9564 - acc: 0.6824 - val_loss: 1.1529 - val_acc: 0.6545\n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 1s 158us/sample - loss: 0.9417 - acc: 0.6676 - val_loss: 1.4741 - val_acc: 0.6845\n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 1s 138us/sample - loss: 0.9747 - acc: 0.6765 - val_loss: 1.2275 - val_acc: 0.6460\n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 1s 145us/sample - loss: 0.9546 - acc: 0.6781 - val_loss: 1.3009 - val_acc: 0.6970\n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 1s 131us/sample - loss: 0.9021 - acc: 0.6811 - val_loss: 1.1359 - val_acc: 0.6770\n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 1s 149us/sample - loss: 0.8947 - acc: 0.6765 - val_loss: 1.6376 - val_acc: 0.6865\n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 1s 135us/sample - loss: 0.8931 - acc: 0.6795 - val_loss: 1.2426 - val_acc: 0.6940\n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 1s 134us/sample - loss: 0.8841 - acc: 0.6779 - val_loss: 1.6259 - val_acc: 0.6590\n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 1s 152us/sample - loss: 0.7795 - acc: 0.6975 - val_loss: 1.5922 - val_acc: 0.6860\n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 1s 139us/sample - loss: 0.7747 - acc: 0.6989 - val_loss: 1.2869 - val_acc: 0.6725\n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 1s 132us/sample - loss: 0.7442 - acc: 0.7013 - val_loss: 1.4125 - val_acc: 0.6855\n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 1s 132us/sample - loss: 0.7652 - acc: 0.7020 - val_loss: 1.7378 - val_acc: 0.6950\n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 1s 128us/sample - loss: 0.7489 - acc: 0.7090 - val_loss: 1.5128 - val_acc: 0.6770\n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 1s 161us/sample - loss: 0.7529 - acc: 0.7044 - val_loss: 1.7734 - val_acc: 0.6865\n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 1s 162us/sample - loss: 0.7248 - acc: 0.7085 - val_loss: 2.2018 - val_acc: 0.7050\n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 1s 133us/sample - loss: 0.7568 - acc: 0.7088 - val_loss: 1.5775 - val_acc: 0.6650\n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 1s 133us/sample - loss: 0.7683 - acc: 0.7056 - val_loss: 1.7718 - val_acc: 0.6970\n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 1s 131us/sample - loss: 0.7573 - acc: 0.7089 - val_loss: 1.6430 - val_acc: 0.6980\n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 1s 136us/sample - loss: 0.6991 - acc: 0.7182 - val_loss: 1.7773 - val_acc: 0.6995\n"
     ]
    }
   ],
   "source": [
    "# links\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule\n",
    "# https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "   initial_lrate = 0.1\n",
    "   drop = 0.5\n",
    "   epochs_drop = 10.0\n",
    "   lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "   return lrate\n",
    "\n",
    "\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "       self.losses = []\n",
    "       self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "       self.losses.append(logs.get('loss'))\n",
    "       self.lr.append(step_decay(len(self.losses)))\n",
    "        \n",
    "loss_history = LossHistory()\n",
    "lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "callbacks_list = [loss_history, lrate]\n",
    "\n",
    "# opt = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "opt = tf.keras.optimizers.RMSprop()\n",
    "model.compile(optimizer=opt,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    x_val, y_val, epochs=30, batch_size=64, callbacks=callbacks_list, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc', 'lr'])\n",
      "[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.0125]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "print(history.history['lr'])\n",
    "# plt.plot(history.history['learning_ra}te'])\n",
    "plt.plot(history.history['lr'])\n",
    "plt.title('learning_rate')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/2\n",
      "47744/48000 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8579\n",
      "Epoch 00001: val_loss improved from inf to 0.41580, saving model to checkpoints/mymodel_1.h5\n",
      "48000/48000 [==============================] - 7s 156us/sample - loss: 0.3865 - acc: 0.8582 - val_loss: 0.4158 - val_acc: 0.8588\n",
      "Epoch 2/2\n",
      "47872/48000 [============================>.] - ETA: 0s - loss: 0.3687 - acc: 0.8668\n",
      "Epoch 00002: val_loss improved from 0.41580 to 0.40554, saving model to checkpoints/mymodel_2.h5\n",
      "48000/48000 [==============================] - 8s 158us/sample - loss: 0.3688 - acc: 0.8668 - val_loss: 0.4055 - val_acc: 0.8594\n"
     ]
    }
   ],
   "source": [
    "# link = https://www.tensorflow.org/tensorboard/scalars_and_keras\n",
    "\n",
    "logdir = \"logs\\\\scalars\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        # The saved model name will include the current epoch.\n",
    "        filepath=\"checkpoints/mymodel_{epoch}.h5\",\n",
    "        save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "    ), \n",
    "    tensorboard_callback\n",
    "]\n",
    "history = model.fit(\n",
    "    x_train, y_train, epochs=2, batch_size=64, callbacks=callbacks, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 11220."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "59808/60000 [============================>.] - ETA: 0s - loss: 0.4761 - acc: 0.8293\n",
      "logs dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
      "60000/60000 [==============================] - 8s 129us/sample - loss: 0.4754 - acc: 0.8295 - val_loss: 0.4338 - val_acc: 0.8457\n",
      "Epoch 2/10\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 0.3581 - acc: 0.8688\n",
      "logs dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
      "60000/60000 [==============================] - 8s 141us/sample - loss: 0.3582 - acc: 0.8688 - val_loss: 0.3559 - val_acc: 0.8737\n",
      "Epoch 3/10\n",
      "59936/60000 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.8819\n",
      "logs dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
      "60000/60000 [==============================] - 9s 146us/sample - loss: 0.3219 - acc: 0.8819 - val_loss: 0.3672 - val_acc: 0.8698\n",
      "Epoch 4/10\n",
      "59936/60000 [============================>.] - ETA: 0s - loss: 0.2983 - acc: 0.8893- E\n",
      "logs dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
      "60000/60000 [==============================] - 8s 138us/sample - loss: 0.2983 - acc: 0.8893 - val_loss: 0.3338 - val_acc: 0.8803\n",
      "Epoch 5/10\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.2806 - acc: 0.8944\n",
      "logs dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
      "60000/60000 [==============================] - 9s 145us/sample - loss: 0.2806 - acc: 0.8945 - val_loss: 0.3379 - val_acc: 0.8786\n",
      "Epoch 6/10\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 0.2651 - acc: 0.9007\n",
      "logs dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
      "60000/60000 [==============================] - 10s 162us/sample - loss: 0.2650 - acc: 0.9008 - val_loss: 0.3449 - val_acc: 0.8799\n",
      "Epoch 7/10\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.9048\n",
      "logs dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
      "60000/60000 [==============================] - 10s 163us/sample - loss: 0.2524 - acc: 0.9048 - val_loss: 0.3763 - val_acc: 0.8695\n",
      "Epoch 8/10\n",
      "59744/60000 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9094\n",
      "logs dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.2431 - acc: 0.9094 - val_loss: 0.3361 - val_acc: 0.8841\n",
      "Epoch 9/10\n",
      "59680/60000 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9117\n",
      "logs dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.2305 - acc: 0.9116 - val_loss: 0.3371 - val_acc: 0.8868\n",
      "Epoch 10/10\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 0.2222 - acc: 0.9158\n",
      "logs dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.2221 - acc: 0.9159 - val_loss: 0.3194 - val_acc: 0.8906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x202897c5198>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    print('\\nlogs', logs.keys())\n",
    "    if(logs.get('acc')>0.60):\n",
    "      print(\"\\nReached 60% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "        \n",
    "callbacks = myCallback()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, callbacks=[callbacks], validation_data= (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
